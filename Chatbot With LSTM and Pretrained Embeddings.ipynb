{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchdata==0.3.0\n",
      "  Downloading torchdata-0.3.0-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 2.9 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.7/site-packages (from torchdata==0.3.0) (1.25.7)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchdata==0.3.0) (2.23.0)\n",
      "Requirement already satisfied: torch==1.11.0 in /opt/conda/lib/python3.7/site-packages (from torchdata==0.3.0) (1.11.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.3.0) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.3.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.3.0) (2.9)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.11.0->torchdata==0.3.0) (3.7.4.1)\n",
      "Installing collected packages: torchdata\n",
      "Successfully installed torchdata-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdata==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.corpus import brown\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from tqdm import tqdm  \n",
    "from torchtext.datasets import SQuAD1\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Output, save, and load brown embeddings\n",
    "\n",
    "model = gensim.models.Word2Vec(brown.sents())\n",
    "model.save('brown.embedding')\n",
    "\n",
    "w2v = gensim.models.Word2Vec.load('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "# Define vocabulary class\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.word_to_idx = {'<sos>': 0, '<eos>': 1, '<unk>': 2, '<pad>': 3}\n",
    "        self.vocab_size = 4\n",
    "        self.idx_to_word = {0: '<sos>', 1: '<eos>', 2: '<unk>', 3: '<pad>'}\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        self.word_count = {}\n",
    "  \n",
    "    def load_word(self, word):\n",
    "        \n",
    "        if type(word) != str:\n",
    "            raise ValueError('The word arg must be a string')\n",
    "        \n",
    "        # Lower case words \n",
    "        word = word.lower()\n",
    "        \n",
    "        if word not in self.word_to_idx:\n",
    "            self.word_to_idx[word] = self.vocab_size\n",
    "            self.idx_to_word[self.vocab_size] = word\n",
    "            self.word_count[word] = 1\n",
    "            self.vocab_size += 1\n",
    "        else:\n",
    "            self.word_count[word] += 1 \n",
    "        \n",
    "    def load_sentence(self, sent):\n",
    "        \n",
    "        if type(sent) != str and type(sent) != list:\n",
    "            raise ValueError('The sent arg must be either a string or a list of strings.')\n",
    "        \n",
    "        if type(sent) == str:\n",
    "            sent = self.tokenizer.tokenize(sent)\n",
    "                    \n",
    "        for word in sent:\n",
    "            self.load_word(word)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.word_to_idx)\n",
    "    \n",
    "    def convert_sentence(self, sent):\n",
    "        \n",
    "        if type(sent) != str and type(sent) != list:\n",
    "            raise ValueError('The sent arg must be either a string or a list of strings.')\n",
    "        \n",
    "        if type(sent) == str:\n",
    "            sent = sent.lower()\n",
    "            sent = self.tokenizer.tokenize(sent)\n",
    "                    \n",
    "        sent_idx = [self.word_to_idx[word] if word in self.word_to_idx else self.word_to_idx['<unk>'] for word in sent ]\n",
    "        \n",
    "        return sent_idx\n",
    "    \n",
    "    def filter_by_freq(self, min_freq=0):\n",
    "        \n",
    "        word_count = {}\n",
    "        word_to_idx = {'<sos>': 0, '<eos>': 1, '<unk>': 2, '<pad>': 3}\n",
    "        idx_to_word = {0: '<sos>', 1: '<eos>', 2: '<unk>', 3: '<pad>'}\n",
    "        vocab_size = 4\n",
    "                \n",
    "        words = [key for key, val in self.word_count.items() if val >= min_freq] \n",
    "\n",
    "        for word in words:\n",
    "            word_to_idx[word] = vocab_size\n",
    "            idx_to_word[vocab_size] = word\n",
    "            vocab_size += 1\n",
    "            \n",
    "            word_count[word] = self.word_count[word]\n",
    "        \n",
    "        # Update state         \n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = idx_to_word\n",
    "        self.word_count = word_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything seems correct!\n"
     ]
    }
   ],
   "source": [
    "# Cell to test the vocab\n",
    "def test_vocab(w2v=None, use_w2v=False):\n",
    "    if use_w2v:\n",
    "        pass\n",
    "    else:\n",
    "        vocab = Vocab()\n",
    "        sentence = \"this is a test sentence with some words. Some words are duplicated for testing\"\n",
    "        vocab.load_sentence(sentence)\n",
    "                \n",
    "        assert vocab.word_count['some'] == 2, \"the word 'some' should appear 2 times\"\n",
    "        assert vocab.word_count['words'] == 2, \"the word 'words' should appear 2 times\"\n",
    "        \n",
    "        assert len(vocab.word_to_idx) == 16, \"the total number of entries in word_to_idx must be 16\"\n",
    "        assert len(vocab.idx_to_word) == 16, \"the total number of entries in idx_to_word must be 16\"\n",
    "        \n",
    "        vocab.filter_by_freq(min_freq=0)\n",
    "        assert len(vocab.word_to_idx) == 16, \"the total number of entries in word_to_idx must be 16\"\n",
    "\n",
    "        vocab.filter_by_freq(min_freq=2)\n",
    "        assert len(vocab.word_to_idx) == 6, \"the total number of entries in word_to_idx must be 6\"\n",
    "            \n",
    "        print(\"Everything seems correct!\")\n",
    "test_vocab()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare loaders\n",
    "\n",
    "def load_vocabulary(train_dataset) -> Vocab:\n",
    "    vocab = Vocab()\n",
    "    \n",
    "    print('Loading vocabulary from the train dataset')\n",
    "    \n",
    "    for context, question, answers, idx_context in tqdm(train_dataset):\n",
    "\n",
    "        vocab.load_sentence(question)\n",
    "\n",
    "        for answer in answers:\n",
    "            vocab.load_sentence(answer)\n",
    "    \n",
    "    print(f\"Original size: {len(vocab)}\")\n",
    "    vocab.filter_by_freq(min_freq=0.0001)\n",
    "    print(f\"After filtering size: {len(vocab)}\")\n",
    "    \n",
    "    return vocab\n",
    "    \n",
    "\n",
    "def reduce_dataset(dataset_in) -> list:\n",
    "    \n",
    "    dataset_out = []\n",
    "    \n",
    "    print('Reduce dataset to consider only Q/A')\n",
    "    for context, question, answers, idx_context in tqdm(dataset_in):\n",
    "\n",
    "        # Select only the first answer         \n",
    "        dataset_out.append((question, answers[0]))\n",
    "\n",
    "    return dataset_out\n",
    "    \n",
    "    \n",
    "def create_dataloader(batch_size: int, train_valid_ratio: float = 0.8) -> (Vocab, DataLoader):\n",
    "    \n",
    "    # Create dataloaders      \n",
    "    dataloders = {'train': None, 'valid': None, 'test': None}\n",
    "    \n",
    "    train_dataset, test_dataset = SQuAD1()\n",
    "\n",
    "    # load vocabulary\n",
    "    vocab = load_vocabulary(train_dataset)\n",
    "      \n",
    "    # Batching the testing dataset\n",
    "    train_dataset = reduce_dataset(train_dataset)\n",
    "    \n",
    "    train_batch_available = len(train_dataset) // batch_size\n",
    "\n",
    "    # Discard the entries not fitting with the batch_size     \n",
    "    train_dataset = train_dataset[:train_batch_available * batch_size]\n",
    "    \n",
    "    # Split the train dataset into training and validation\n",
    "    train_batch_num = int((len(train_dataset) / batch_size) * train_valid_ratio)\n",
    "    train_num = train_batch_num * batch_size\n",
    "\n",
    "    valid_num = int((len(train_dataset) - train_num))\n",
    "\n",
    "    train_dataset, valid_dataset = data.random_split(train_dataset, [train_num, valid_num])\n",
    "    \n",
    "    # Batching the testing dataset     \n",
    "  \n",
    "    test_dataset = reduce_dataset(test_dataset)\n",
    "    test_batch_available = len(test_dataset) // batch_size\n",
    "\n",
    "    # Discard the entries not fitting with the batch_size     \n",
    "    test_dataset = test_dataset[:test_batch_available * batch_size]\n",
    "    \n",
    "    # Populate the dataloaders\n",
    "\n",
    "    dataloders['train'] = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) \n",
    "    dataloders['valid'] = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False) \n",
    "    dataloders['test'] = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) \n",
    "\n",
    "    \n",
    "    return vocab, dataloders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCH_NUM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocabulary from the train dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87599it [00:08, 10567.09it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 52602\n",
      "After filtering size: 52602\n",
      "Reduce dataset to consider only Q/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87599it [00:03, 25056.94it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce dataset to consider only Q/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10570it [00:00, 13500.92it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab, dl = create_dataloader(BATCH_SIZE, train_valid_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDF(path):\n",
    "    '''\n",
    "\n",
    "    You will use this function to load the dataset into a Pandas Dataframe for processing.\n",
    "\n",
    "    '''\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_text(sentence):\n",
    "    \n",
    "    '''\n",
    "\n",
    "    Our text needs to be cleaned with a tokenizer. This function will perform that task.\n",
    "    https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "    '''\n",
    "    \n",
    "    tokens = nltk.tokenize.word_tokenize(s) \n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "def train_test_split(SRC, TRG):\n",
    "    \n",
    "    '''\n",
    "    Input: SRC, our list of questions from the dataset\n",
    "            TRG, our list of responses from the dataset\n",
    "\n",
    "    Output: Training and test datasets for SRC & TRG\n",
    "\n",
    "    '''\n",
    "    \n",
    "    return SRC_train_dataset, SRC_test_dataset, TRG_train_dataset, TRG_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "oQLTP2Wmi1eB"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding_size = 100, dropout=0.5):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size \n",
    "        \n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        \n",
    "        # self.lstm, accepts the vectorized input and passes a hidden state\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, num_layers=2, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        '''\n",
    "        Inputs: src, the src vector\n",
    "        Outputs: output, the encoder outputs\n",
    "                 self.hidden, the hidden state\n",
    "                 self.cell_state, the cell state\n",
    "        '''\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        output, (hidden, cell_state) = self.lstm(embedded)\n",
    "            \n",
    "        return output, hidden, cell_state\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, hidden_size, output_size, embedding_size=100, dropout=0.5):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "                \n",
    "        # self.embedding provides a vector representation of the target to our model\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, self.embedding_size)\n",
    "\n",
    "        # self.lstm, accepts the embeddings and outputs a hidden state\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, num_layers=2, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # self.ouput, predicts on the hidden state via a linear output layer\n",
    "        self.fcl = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, trg, hidden, cell_state):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the target vector\n",
    "        Outputs: o, the prediction\n",
    "                h, the hidden state\n",
    "        '''        \n",
    "        \n",
    "        embedded = self.dropout(self.embedding(trg))\n",
    "\n",
    "        output, (hidden, cell_state) = self.lstm(embedded, (hidden, cell_state))\n",
    "\n",
    "        output = self.fcl(output.view(-1, self.hidden_size))\n",
    "\n",
    "        return output, (hidden, cell_state)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_input_size, encoder_hidden_size, decoder_hidden_size, decoder_output_size, device):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.encoder = Encoder(encoder_input_size, encoder_hidden_size).to(device)\n",
    "        self.decoder = Decoder(decoder_hidden_size, decoder_output_size).to(device)\n",
    "        \n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):      \n",
    "               \n",
    "        # Encoder\n",
    "        output, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # Define tensor to store decoder outputs\n",
    "        decoder_outputs = torch.zeros(trg.shape[0], trg.shape[1], self.decoder.output_size).to(self.device)\n",
    "        \n",
    "        best_output = trg[:, 0]\n",
    "\n",
    "        for trg_col in range(trg.shape[1]):\n",
    "\n",
    "            # Teaching modification\n",
    "            trg_input =  trg[:, trg_col] if random.random() < teacher_forcing_ratio else best_output\n",
    "            trg_input = trg_input.unsqueeze(1)\n",
    "                                        \n",
    "            output, (hidden, cell) = self.decoder(trg_input, hidden, cell)\n",
    "\n",
    "            best_output = output.argmax(1)\n",
    "            \n",
    "            decoder_outputs[:, trg_col] = output\n",
    "\n",
    "                \n",
    "        return decoder_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_wrap_and_pad(batch_data, vocab: Vocab, max_length: int = 10, reverse=False) -> torch.LongTensor:\n",
    "\n",
    "    # Convert to index\n",
    "    batch_data = [vocab.convert_sentence(sentence) for sentence in batch_data]\n",
    "    \n",
    "    # Reverse order of the batched data to remove short memory connections\n",
    "    if reverse is True:\n",
    "        for sentence in batch_data:\n",
    "            sentence.reverse()\n",
    "            \n",
    "    # Compute max lengths for the padding             \n",
    "    batch_max_length = min(max([len(sentence) for sentence in batch_data]), max_length)\n",
    "\n",
    "    # define two functions: one for wrapping the sentence with the sos and eos tokens, \n",
    "    # and one for padding the sentence when necesary\n",
    "    wrapper = lambda x: [vocab.word_to_idx['<sos>']] + x[:min(len(x), batch_max_length - 2)] + [vocab.word_to_idx['<eos>']]\n",
    "    padding = lambda x: x[:min(len(x), batch_max_length )] + [vocab.word_to_idx['<pad>']] * max(batch_max_length -  min(len(x), batch_max_length + 2), 0)\n",
    "\n",
    "    return torch.LongTensor([padding(wrapper(sentence)) for sentence in batch_data])  \n",
    "\n",
    "def convert_to_text(data, vocab: Vocab) -> str:\n",
    "\n",
    "    text = ''\n",
    "    for element in data:\n",
    "        text += vocab.idx_to_word[int(element)] + \" \"\n",
    "    \n",
    "    return text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everthing seems ok!\n"
     ]
    }
   ],
   "source": [
    "# Test wrapping and padding\n",
    "def test_wrap_and_pad(vocab):\n",
    "    batch_data = ['this is a sentence', 'this is a very very very long sentence', 'this is a very very super uber mega ultra useless long sentence']\n",
    "    \n",
    "    batch_wp = batch_wrap_and_pad(batch_data, vocab)\n",
    "        \n",
    "    assert batch_wp[0][0] == 0 and batch_wp[1][0] == 0, \"No <sos> token\" \n",
    "    assert batch_wp[0][5] == 1, \"No <eos> token\"\n",
    "    assert batch_wp[1][9] == 1, \"No <eos> token\"\n",
    "    assert batch_wp[0][6] == 3 and batch_wp[0][7] == 3 and batch_wp[0][8] == 3 and batch_wp[0][9] == 3, \"No <pad> token\"\n",
    "    \n",
    "    batch_wp_rev = batch_wrap_and_pad(batch_data, vocab, reverse=True)\n",
    "    \n",
    "    assert all(torch.eq(batch_wp[0][1:5], batch_wp_rev[0][1:5].flip(dims=(0,)))), \"The first sentence must be equal but reversed\"\n",
    "    assert all(torch.eq(batch_wp[1][1:9], batch_wp_rev[1][1:9].flip(dims=(0,)))), \"The second sentence must be equal but reversed\"\n",
    "    \n",
    "    print(\"Everthing seems ok!\")\n",
    "    \n",
    "test_wrap_and_pad(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch_num, dl, vocab, model):\n",
    "\n",
    "    # Define epoch losses\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "\n",
    "    # Define optimiser     \n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = vocab.word_to_idx['<pad>'])\n",
    "    \n",
    "    # Gradient clipping\n",
    "    clip = 1.0\n",
    "    \n",
    "    for epoch in range(1, epoch_num + 1):\n",
    "        \n",
    "        # Enable training\n",
    "        model.train()\n",
    "        \n",
    "        # Training\n",
    "        for src, trg in tqdm(dl['train']):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Wrap and pad the batch             \n",
    "            src = batch_wrap_and_pad(src, vocab, reverse=True)\n",
    "            trg = batch_wrap_and_pad(trg, vocab)\n",
    "        \n",
    "            # Move to cuda if available             \n",
    "            if torch.cuda.is_available():\n",
    "                src, trg = src.cuda(), trg.cuda()\n",
    "            \n",
    "            # Get seq2seq output             \n",
    "            output = model(src, trg)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            \n",
    "            # Compute loss     \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradient     \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Disable training\n",
    "        model.eval()\n",
    "        \n",
    "        # Validation              \n",
    "        for src, trg in tqdm(dl['valid']):\n",
    "                    \n",
    "            # Wrap and pad the batch             \n",
    "            src = batch_wrap_and_pad(src, vocab, reverse=True)\n",
    "            trg = batch_wrap_and_pad(trg, vocab)\n",
    "        \n",
    "            # Move to cuda if available             \n",
    "            if torch.cuda.is_available():\n",
    "                src, trg = src.cuda(), trg.cuda()\n",
    "            \n",
    "            # Get seq2seq output             \n",
    "            output = model(src, trg)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            # Compute loss     \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "#         if epoch % 1:\n",
    "        print(f'Epoch: {epoch}, Training loss: {train_loss}, Valid loss: {valid_loss}')\n",
    "            \n",
    "            \n",
    "def testing(epoch_numb, dl, vocab, model):\n",
    "    \n",
    "    # Enable training\n",
    "    model.eval()\n",
    "    \n",
    "    # Define epoch loss\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Define optimiser     \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for epoch in range(1, epoch_numb + 1):\n",
    "\n",
    "            for src, trg in tqdm(dl['test']):\n",
    "\n",
    "                src = batch_wrap_and_pad(src, vocab)\n",
    "                trg = batch_wrap_and_pad(trg, vocab)\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "                output = model(src, trg)\n",
    "\n",
    "                output_dim = output.shape[-1]\n",
    "\n",
    "                output = output[:, 1:].reshape(-1, output_dim)\n",
    "                trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "                # Compute loss     \n",
    "                loss = criterion(output, trg)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "            print(f'Epoch loss {epoch_loss}')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/547 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the European equivalent of \"Qi\"?\n",
      "<sos> qi of equivalent european the is what <eos> <pad> \n",
      "<sos> pneuma <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 1/547 [00:30<4:38:00, 30.55s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is torque determined?\n",
      "<sos> determined torque is how <eos> <pad> <pad> <pad> <pad> \n",
      "<sos> from the vector product of the interacting fields <eos> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 2/547 [00:52<4:13:48, 27.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who was criticized for not handling the Forsyth case independently?\n",
      "<sos> independently case forsyth the handling not for criticized <eos> \n",
      "<sos> the head master <eos> <pad> <pad> <pad> <pad> <pad> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 3/547 [01:12<3:50:59, 25.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first private company to offer Hyderabad internet service began offering it in what year?\n",
      "<sos> year what in it offering began service internet <eos> \n",
      "<sos> 1998 <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 4/547 [01:31<3:33:23, 23.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who benefits from political corruption?\n",
      "<sos> corruption political from benefits who <eos> <pad> <pad> <pad> \n",
      "<sos> government officials <eos> <pad> <pad> <pad> <pad> <pad> <pad> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_input_size = len(vocab)\n",
    "encoder_hidden_size = 512\n",
    "\n",
    "decoder_hidden_size = 512\n",
    "decoder_output_size = len(vocab)\n",
    "\n",
    "# Check cuda availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Seq2Seq(encoder_input_size, encoder_hidden_size, decoder_hidden_size, decoder_output_size, device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "train(EPOCH_NUM, dl, vocab, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
