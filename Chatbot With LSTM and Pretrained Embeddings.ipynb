{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchdata==0.3.0\n",
      "  Downloading torchdata-0.3.0-py3-none-any.whl (47 kB)\n",
      "\u001B[K     |████████████████████████████████| 47 kB 2.9 MB/s eta 0:00:011\n",
      "\u001B[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchdata==0.3.0) (2.23.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.7/site-packages (from torchdata==0.3.0) (1.25.7)\n",
      "Requirement already satisfied: torch==1.11.0 in /opt/conda/lib/python3.7/site-packages (from torchdata==0.3.0) (1.11.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.3.0) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.3.0) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchdata==0.3.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.11.0->torchdata==0.3.0) (3.7.4.1)\n",
      "Installing collected packages: torchdata\n",
      "Successfully installed torchdata-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdata==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.corpus import brown\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from tqdm import tqdm  \n",
    "from torchtext.datasets import SQuAD1\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eg81uNTWixbi",
    "outputId": "9c0f9eda-75fb-4526-e9b6-f9a76eeeb007",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Output, save, and load brown embeddings\n",
    "\n",
    "model = gensim.models.Word2Vec(brown.sents())\n",
    "model.save('brown.embedding')\n",
    "\n",
    "w2v = gensim.models.Word2Vec.load('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "# Define vocabulary class\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.word_to_idx = {'<sos>': 0, '<eos>': 1, '<unk>': 2, '<pad>': 3}\n",
    "        self.vocab_size = 4\n",
    "        self.idx_to_word = {0: '<sos>', 1: '<eos>', 2: '<unk>', 3: '<pad>'}\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        self.porter_stem = PorterStemmer()\n",
    "        self.word_count = {}\n",
    "        self.default_token = ('<unk>', 2)\n",
    "              \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.word_to_idx)\n",
    "    \n",
    "    def clean_text(self, sentence: str) -> list:\n",
    "        \n",
    "        assert type(sentence) == str, 'The word arg must be a string'\n",
    "        \n",
    "        return [self.porter_stem.stem(word) for word in self.tokenizer.tokenize(sentence)]\n",
    "\n",
    "    def _load_word(self, word: str) -> None:\n",
    "        \n",
    "        assert type(word) == str, 'The word arg must be a string'\n",
    "        \n",
    "        if word not in self.word_to_idx:\n",
    "            self.word_to_idx[word] = self.vocab_size\n",
    "            self.idx_to_word[self.vocab_size] = word\n",
    "            self.word_count[word] = 1\n",
    "            self.vocab_size += 1\n",
    "        else:\n",
    "            self.word_count[word] += 1 \n",
    "        \n",
    "    def load(self, dataset) -> None:\n",
    "        \n",
    "        for question, answer in tqdm(dataset):\n",
    "\n",
    "            self._load_sentence(question)\n",
    "\n",
    "            self._load_sentence(answer)\n",
    "            \n",
    "    def _load_sentence(self, sentence: str) -> None:\n",
    "        \n",
    "        assert type(sentence) == str, 'The sentence arg must be a string'\n",
    "        \n",
    "        sentence = self.clean_text(sentence)\n",
    "        \n",
    "        for word in sentence:\n",
    "            self._load_word(word)\n",
    "    \n",
    "    def filter_min_freq(self, min_freq: int = 0) -> None:\n",
    "        \n",
    "        word_count = {}\n",
    "        word_to_idx = {'<sos>': 0, '<eos>': 1, '<unk>': 2, '<pad>': 3}\n",
    "        idx_to_word = {0: '<sos>', 1: '<eos>', 2: '<unk>', 3: '<pad>'}\n",
    "        vocab_size = 4\n",
    "                \n",
    "        words = [key for key, val in self.word_count.items() if val >= min_freq] \n",
    "\n",
    "        for word in words:\n",
    "            word_to_idx[word] = vocab_size\n",
    "            idx_to_word[vocab_size] = word\n",
    "            vocab_size += 1\n",
    "            \n",
    "            word_count[word] = self.word_count[word]\n",
    "        \n",
    "        # Update state         \n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.idx_to_word = idx_to_word\n",
    "        self.word_count = word_count\n",
    "\n",
    "    def stoi(self, sentence: str) -> list:\n",
    "        \n",
    "        assert type(sentence) == str, 'The word arg must be a string'\n",
    "        \n",
    "        sentence = self.clean_text(sentence)\n",
    "                    \n",
    "        return [self.word_to_idx[word] if word in self.word_to_idx else self.word_to_idx[self.default_token[0]] for word in sentence ]\n",
    "\n",
    "    def itos(self, sentence: list) -> list:\n",
    "        \n",
    "        assert type(sentence) == list, 'The word arg must be a list of ints'\n",
    "                    \n",
    "        return [self.idx_to_word[word_idx] if word_idx in self.idx_to_word else self.idx_to_word[self.default_token[1]] for word_idx in sentence ]\n",
    "    \n",
    "    \n",
    "    def _batch_wrap_and_pad(self, sentence: list, max_length: int, reverse=False) -> torch.LongTensor:\n",
    "\n",
    "        # Reverse order of the batched data to remove short memory connections\n",
    "        if reverse is True:\n",
    "            sentence.reverse()\n",
    "            \n",
    "        # define two functions: one for wrapping the sentence with the sos and eos tokens, \n",
    "        # and one for padding the sentence when necesary\n",
    "\n",
    "        wrapper = lambda x: [self.word_to_idx['<sos>']] + x[:min(len(x), max_length - 2)] + [self.word_to_idx['<eos>']]\n",
    "        padding = lambda x: x[:min(len(x), max_length )] + [self.word_to_idx['<pad>']] * max(max_length -  min(len(x), max_length + 2), 0)\n",
    "\n",
    "        return padding(wrapper(sentence))\n",
    "\n",
    "    def collate_fcn(self, batch):\n",
    "        \n",
    "        # Compute max lengths for the padding             \n",
    "        max_length = 10\n",
    "            \n",
    "        total_lengths = []\n",
    "        \n",
    "        for (question, answer) in batch:\n",
    "            \n",
    "            # Convert to index\n",
    "            question = self.stoi(question)\n",
    "            answer = self.stoi(answer)\n",
    "            \n",
    "            total_lengths.append(len(answer))\n",
    "            total_lengths.append(len(question))\n",
    "                \n",
    "        batch_max_length = min(max(total_lengths), max_length)\n",
    "\n",
    "        src = torch.zeros(size=(len(batch), batch_max_length), dtype=torch.int64)\n",
    "        trg = torch.zeros(size=(len(batch), batch_max_length), dtype=torch.int64)\n",
    "        \n",
    "        for idx_batch, (question, answer) in enumerate(batch):\n",
    "            \n",
    "            # Convert to index\n",
    "            question = self.stoi(question)\n",
    "            answer = self.stoi(answer)\n",
    "            \n",
    "            src[idx_batch, :] = torch.LongTensor(self._batch_wrap_and_pad(question, max_length=batch_max_length, reverse=True))\n",
    "            trg[idx_batch, :] = torch.LongTensor(self._batch_wrap_and_pad(answer, max_length=batch_max_length))\n",
    "\n",
    "\n",
    "        return (src, trg)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-filtering\n",
      "\n",
      "[4, 5, 6, 7, 8, 9, 10, 11, 10, 11, 12, 13, 14, 7]\n",
      "thi is a test sentenc with some word some word are duplic for test\n",
      "\n",
      "Post-filtering min_freq == 2\n",
      "\n",
      "[2, 2, 2, 4, 2, 2, 5, 6, 5, 6, 2, 2, 2, 4]\n",
      "<unk> <unk> <unk> test <unk> <unk> some word some word <unk> <unk> <unk> test\n",
      "\n",
      "Everything seems correct!\n"
     ]
    }
   ],
   "source": [
    "# Cell to test the vocab\n",
    "def test_vocab(w2v=None, use_w2v=False):\n",
    "    if use_w2v:\n",
    "        pass\n",
    "    else:\n",
    "        vocab = Vocab()\n",
    "        sentence = \"this is a test sentence with some words. Some words are duplicated for testing\"\n",
    "        vocab._load_sentence(sentence)\n",
    "                \n",
    "        assert vocab.word_count['some'] == 2, \"the word 'some' should appear 2 times\"\n",
    "        assert vocab.word_count['word'] == 2, \"the word 'words' should appear 2 times\"\n",
    "        \n",
    "        assert len(vocab.word_to_idx) == 15, \"the total number of entries in word_to_idx must be 15\"\n",
    "        assert len(vocab.idx_to_word) == 15, \"the total number of entries in idx_to_word must be 15\"\n",
    "\n",
    "        print(\"Pre-filtering\\n\")\n",
    "        sentence_idx = vocab.stoi(sentence)\n",
    "        print(sentence_idx)\n",
    "        sentence = ' '.join(vocab.itos(sentence_idx))\n",
    "        print(sentence)\n",
    "        \n",
    "        vocab.filter_min_freq(min_freq=0)\n",
    "        assert len(vocab.word_to_idx) == 15, \"the total number of entries in word_to_idx must be 16\"\n",
    "\n",
    "        vocab.filter_min_freq(min_freq=2)\n",
    "        assert len(vocab.word_to_idx) == 7, \"the total number of entries in word_to_idx must be 6\"\n",
    "        \n",
    "        print(\"\\nPost-filtering min_freq == 2\\n\")\n",
    "        sentence_idx = vocab.stoi(sentence)\n",
    "        print(sentence_idx)\n",
    "        sentence = ' '.join(vocab.itos(sentence_idx))\n",
    "        print(sentence)\n",
    "            \n",
    "            \n",
    "        batch = [(\"question 1, really\", \"answ 1, nope nope nope nope nope v v nope nope\"), (\"question 2, GM\", \"answ 2, nope sure yes\"), (\"question 3, heila\", \"answ 3, nope\")]\n",
    "            \n",
    "        (src, trg) = vocab.collate_fcn(batch)\n",
    "        \n",
    "        assert src.shape[0] == 3, \"The src(x,) must have the same dimension of the batch\"\n",
    "        assert src.shape[1] == 10, \"The src(,x) must have the same dimension of the maximum allowed sentence = 10\"\n",
    "        assert trg.shape[0] == 3, \"The trg(x,) must have the same dimension of the batch\"\n",
    "        assert trg.shape[1] == 10, \"The trg(,x) must have the same dimension of the maximum allowed sentence = 10\"\n",
    "            \n",
    "        print(\"\\nEverything seems correct!\")\n",
    "\n",
    "test_vocab()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dataset(dataset_in) -> list:\n",
    "    \n",
    "    dataset_out = []\n",
    "    \n",
    "    print('Reduce dataset to consider only Q/A')\n",
    "    for context, question, answers, idx_context in tqdm(dataset_in):\n",
    "\n",
    "        # Select only the first answer         \n",
    "        dataset_out.append((question, answers[0]))\n",
    "\n",
    "    return dataset_out\n",
    "    \n",
    "    \n",
    "def create_dataloader(batch_size: int, train_valid_ratio: float = 0.8) -> (Vocab, DataLoader):\n",
    "    \n",
    "    # Create dataloaders      \n",
    "    dataloders = {'train': None, 'valid': None, 'test': None}\n",
    "    \n",
    "    train_dataset, test_dataset = SQuAD1()\n",
    "      \n",
    "    # Batching the testing dataset\n",
    "    train_dataset = reduce_dataset(train_dataset)\n",
    "    \n",
    "    train_batch_available = len(train_dataset) // batch_size\n",
    "\n",
    "    # Discard the entries not fitting with the batch_size     \n",
    "    train_dataset = train_dataset[:train_batch_available * batch_size]\n",
    "    \n",
    "    # Split the train dataset into training and validation\n",
    "    train_batch_num = int((len(train_dataset) / batch_size) * train_valid_ratio)\n",
    "    train_num = train_batch_num * batch_size\n",
    "\n",
    "    valid_num = int((len(train_dataset) - train_num))\n",
    "\n",
    "    train_dataset, valid_dataset = data.random_split(train_dataset, [train_num, valid_num])\n",
    "    \n",
    "    # Batching the testing dataset     \n",
    "  \n",
    "    test_dataset = reduce_dataset(test_dataset)\n",
    "    test_batch_available = len(test_dataset) // batch_size\n",
    "\n",
    "    # Discard the entries not fitting with the batch_size     \n",
    "    test_dataset = test_dataset[:test_batch_available * batch_size]\n",
    "    \n",
    "    # Create vocabulary\n",
    "    vocab = Vocab()\n",
    "    vocab.load(train_dataset)\n",
    "    vocab.filter_min_freq(min_freq=10)\n",
    "    print(f\"Final vocabulary size: {len(vocab)}\")\n",
    "    \n",
    "    # Populate the dataloaders\n",
    "\n",
    "    dataloders['train'] = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=vocab.collate_fcn) \n",
    "    dataloders['valid'] = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=vocab.collate_fcn ) \n",
    "    dataloders['test'] = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=vocab.collate_fcn) \n",
    "    \n",
    "    return vocab, dataloders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCH_NUM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce dataset to consider only Q/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87599it [00:02, 29200.44it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce dataset to consider only Q/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10570it [00:00, 20128.97it/s]\n",
      "100%|██████████| 70016/70016 [00:34<00:00, 2012.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final vocabulary size: 6226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab, dl = create_dataloader(BATCH_SIZE, train_valid_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oQLTP2Wmi1eB"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding_size = 100, dropout=0.5):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size \n",
    "        \n",
    "        # self.embedding provides a vector representation of the inputs to our model\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        \n",
    "        # self.lstm, accepts the vectorized input and passes a hidden state\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, num_layers=2, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        '''\n",
    "        Inputs: src, the src vector\n",
    "        Outputs: output, the encoder outputs\n",
    "                 self.hidden, the hidden state\n",
    "                 self.cell_state, the cell state\n",
    "        '''\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        output, (hidden, cell_state) = self.lstm(embedded)\n",
    "            \n",
    "        return output, hidden, cell_state\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "      \n",
    "    def __init__(self, hidden_size, output_size, embedding_size=100, dropout=0.5):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "                \n",
    "        # self.embedding provides a vector representation of the target to our model\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, self.embedding_size)\n",
    "\n",
    "        # self.lstm, accepts the embeddings and outputs a hidden state\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, num_layers=2, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # self.ouput, predicts on the hidden state via a linear output layer\n",
    "        self.fcl = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, trg, hidden, cell_state):\n",
    "        \n",
    "        '''\n",
    "        Inputs: i, the target vector\n",
    "        Outputs: o, the prediction\n",
    "                h, the hidden state\n",
    "        '''        \n",
    "        \n",
    "        embedded = self.dropout(self.embedding(trg))\n",
    "\n",
    "        output, (hidden, cell_state) = self.lstm(embedded, (hidden, cell_state))\n",
    "\n",
    "        output = self.fcl(output.view(-1, self.hidden_size))\n",
    "\n",
    "        return output, (hidden, cell_state)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_input_size, encoder_hidden_size, decoder_hidden_size, decoder_output_size, device):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.encoder = Encoder(encoder_input_size, encoder_hidden_size).to(device)\n",
    "        self.decoder = Decoder(decoder_hidden_size, decoder_output_size).to(device)\n",
    "        \n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):      \n",
    "               \n",
    "        # Encoder\n",
    "        output, hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # Define tensor to store decoder outputs\n",
    "        decoder_outputs = torch.zeros(trg.shape[0], trg.shape[1], self.decoder.output_size).to(self.device)\n",
    "        \n",
    "        best_output = trg[:, 0]\n",
    "\n",
    "        for trg_col in range(trg.shape[1]):\n",
    "\n",
    "            # Teaching modification\n",
    "            trg_input =  trg[:, trg_col] if random.random() < teacher_forcing_ratio else best_output\n",
    "            trg_input = trg_input.unsqueeze(1)\n",
    "                                        \n",
    "            output, (hidden, cell) = self.decoder(trg_input, hidden, cell)\n",
    "\n",
    "            best_output = output.argmax(1)\n",
    "            \n",
    "            decoder_outputs[:, trg_col] = output\n",
    "\n",
    "                \n",
    "        return decoder_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch_num, dl, model, optimiser, criterion):\n",
    "\n",
    "    # Define epoch losses\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    \n",
    "    # Gradient clipping\n",
    "    clip = 1.0\n",
    "    \n",
    "    for epoch in range(1, epoch_num + 1):\n",
    "        \n",
    "        # Enable training\n",
    "        model.train()\n",
    "        \n",
    "        # Training\n",
    "        for src, trg in tqdm(dl['train']):\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "        \n",
    "            # Move to cuda if available             \n",
    "            if torch.cuda.is_available():\n",
    "                src, trg = src.cuda(), trg.cuda()\n",
    "            \n",
    "            # Get seq2seq output             \n",
    "            output = model(src, trg)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "            \n",
    "            # Compute loss     \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip gradient     \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            optimiser.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Disable training\n",
    "        model.eval()\n",
    "        \n",
    "        # Validation              \n",
    "        for src, trg in tqdm(dl['valid']):\n",
    "        \n",
    "            # Move to cuda if available             \n",
    "            if torch.cuda.is_available():\n",
    "                src, trg = src.cuda(), trg.cuda()\n",
    "            \n",
    "            # Get seq2seq output             \n",
    "            output = model(src, trg)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            # Compute loss     \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "\n",
    "        size_train = len(dl['train'])\n",
    "        size_valid = len(dl['valid'])\n",
    "#         if epoch % 1:\n",
    "        print(f\"Epoch: {epoch}, Training loss: {train_loss / size_train}, Valid loss: {valid_loss / size_valid}\")\n",
    "            \n",
    "            \n",
    "def testing(epoch_numb, dl, vocab, model):\n",
    "    \n",
    "    # Enable training\n",
    "    model.eval()\n",
    "    \n",
    "    # Define epoch loss\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Define optimiser     \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for epoch in range(1, epoch_numb + 1):\n",
    "\n",
    "            for src, trg in tqdm(dl['test']):\n",
    "\n",
    "                src = batch_wrap_and_pad(src, vocab)\n",
    "                trg = batch_wrap_and_pad(trg, vocab)\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "                output = model(src, trg)\n",
    "\n",
    "                output_dim = output.shape[-1]\n",
    "\n",
    "                output = output[:, 1:].reshape(-1, output_dim)\n",
    "                trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "                # Compute loss     \n",
    "                loss = criterion(output, trg)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "            print(f'Epoch loss {epoch_loss}')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [01:57<00:00,  4.67it/s]\n",
      "100%|██████████| 137/137 [00:21<00:00,  6.38it/s]\n",
      "  0%|          | 0/547 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training loss: 2.5142471831720967, Valid loss: 2.2129025520199406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 547/547 [01:57<00:00,  4.68it/s]\n",
      "100%|██████████| 137/137 [00:21<00:00,  6.33it/s]\n",
      "  0%|          | 0/547 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training loss: 4.703420105538377, Valid loss: 4.367922130292349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 317/547 [01:07<00:48,  4.73it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-dd2a4284db51>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mEPOCH_NUM\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvocab\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimiser\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-14-9650c9c69f7b>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(epoch_num, dl, vocab, model, optimiser, criterion)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m         \u001B[0;31m# Training\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdl\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'train'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m             \u001B[0moptimiser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/std.py\u001B[0m in \u001B[0;36m__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1106\u001B[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001B[1;32m   1107\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1108\u001B[0;31m         \u001B[0;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[0;32min\u001B[0m \u001B[0miterable\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1109\u001B[0m             \u001B[0;32myield\u001B[0m \u001B[0mobj\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1110\u001B[0m             \u001B[0;31m# Update and possibly print the progressbar.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    528\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    529\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 530\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    531\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    532\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[0;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    568\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    569\u001B[0m         \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 570\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    571\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    572\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     50\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 52\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollate_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-3-2795cd477bf0>\u001B[0m in \u001B[0;36mcollate_fcn\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    114\u001B[0m             \u001B[0;31m# Convert to index\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 115\u001B[0;31m             \u001B[0mquestion\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstoi\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mquestion\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    116\u001B[0m             \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstoi\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-3-2795cd477bf0>\u001B[0m in \u001B[0;36mstoi\u001B[0;34m(self, sentence)\u001B[0m\n\u001B[1;32m     78\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'The word arg must be a string'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     79\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 80\u001B[0;31m         \u001B[0msentence\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclean_text\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     81\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     82\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mword_to_idx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mword_to_idx\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mword_to_idx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdefault_token\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msentence\u001B[0m \u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-3-2795cd477bf0>\u001B[0m in \u001B[0;36mclean_text\u001B[0;34m(self, sentence)\u001B[0m\n\u001B[1;32m     22\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'The word arg must be a string'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mporter_stem\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_load_word\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mword\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-3-2795cd477bf0>\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     22\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'The word arg must be a string'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mporter_stem\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mword\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_load_word\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mword\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/nltk/stem/porter.py\u001B[0m in \u001B[0;36mstem\u001B[0;34m(self, word, to_lowercase)\u001B[0m\n\u001B[1;32m    669\u001B[0m         \u001B[0mstem\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_step2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    670\u001B[0m         \u001B[0mstem\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_step3\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 671\u001B[0;31m         \u001B[0mstem\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_step4\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    672\u001B[0m         \u001B[0mstem\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_step5a\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    673\u001B[0m         \u001B[0mstem\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_step5b\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/nltk/stem/porter.py\u001B[0m in \u001B[0;36m_step4\u001B[0;34m(self, word)\u001B[0m\n\u001B[1;32m    594\u001B[0m                 \u001B[0;34m(\u001B[0m\u001B[0;34m\"ous\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmeasure_gt_1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    595\u001B[0m                 \u001B[0;34m(\u001B[0m\u001B[0;34m\"ive\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmeasure_gt_1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 596\u001B[0;31m                 \u001B[0;34m(\u001B[0m\u001B[0;34m\"ize\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmeasure_gt_1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    597\u001B[0m             ],\n\u001B[1;32m    598\u001B[0m         )\n",
      "\u001B[0;32m/opt/conda/lib/python3.7/site-packages/nltk/stem/porter.py\u001B[0m in \u001B[0;36m_apply_rule_list\u001B[0;34m(self, word, rules)\u001B[0m\n\u001B[1;32m    261\u001B[0m                     \u001B[0;31m# Don't try any further rules\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    262\u001B[0m                     \u001B[0;32mreturn\u001B[0m \u001B[0mword\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 263\u001B[0;31m             \u001B[0;32mif\u001B[0m \u001B[0mword\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mendswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msuffix\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    264\u001B[0m                 \u001B[0mstem\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_replace_suffix\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuffix\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    265\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mcondition\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mcondition\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "encoder_input_size = len(vocab)\n",
    "encoder_hidden_size = 256\n",
    "\n",
    "decoder_hidden_size = 256\n",
    "decoder_output_size = len(vocab)\n",
    "\n",
    "# Check cuda availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define model\n",
    "model = Seq2Seq(encoder_input_size, encoder_hidden_size, decoder_hidden_size, decoder_output_size, device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Define optimiser and criterion\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.7)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = vocab.stoi('<pad>')[0])\n",
    "\n",
    "\n",
    "train(EPOCH_NUM, dl, model, optimiser, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
